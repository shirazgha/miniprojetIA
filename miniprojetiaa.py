# -*- coding: utf-8 -*-
"""MiniprojetIAA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/Ghizaoui/ProjectAII/blob/main/MiniprojetIAA.ipynb
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from PIL import Image
from glob import glob
 
from sklearn.model_selection import train_test_split
from sklearn import metrics
 
import cv2
import gc
import os
 
import tensorflow as tf
from tensorflow import keras
from keras import layers

 
 
import warnings
warnings.filterwarnings('ignore')
import pydot
import pydotplus




path = "C:\\Users\\asus\\DD\\train"
classes = os.listdir(path)
#classes =['BEN', 'CAN','NOR']



IMG_SIZE = 256
SPLIT = 0.2
EPOCHS = 10
BATCH_SIZE = 64

X = []
Y = []
 
for i, cat in enumerate(classes):
  images = glob(f'{path}\\{cat}\\*.*')
 

  for image in images:
   print(image)
   img = cv2.imread(image)	 
   if img is None:
      print('Wrong path:', path)
   else:
    img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))
    X.append(img)
    Y.append(i)


X = np.asarray(X)
one_hot_encoded_Y = pd.get_dummies(Y).values
X

X_train, X_val, Y_train, Y_val = train_test_split(X, 
                                                  one_hot_encoded_Y,
                                                  test_size = SPLIT,
                                                  random_state = 2022)
print(X_train.shape, X_val.shape)
def modele():
 
    model = keras.models.Sequential([
            layers.Conv2D(filters=32,
                      kernel_size=(5, 5),
                      activation='relu',
                      input_shape=(IMG_SIZE,
                                   IMG_SIZE,
                                   3),
                      padding='same'),
        layers.MaxPooling2D(2, 2),
 
        layers.Conv2D(filters=64,
                      kernel_size=(3, 3),
                      activation='relu',
                      padding='same'),
        layers.MaxPooling2D(2, 2),
     
        layers.Conv2D(filters=128,
                      kernel_size=(3, 3),
                      activation='relu',
                      padding='same'),
        layers.MaxPooling2D(2, 2),
     
        layers.Flatten(),
        layers.Dense(256, activation='relu'),
        layers.BatchNormalization(),
        layers.Dense(128, activation='relu'),
        layers.Dropout(0.3),
        layers.BatchNormalization(),
        layers.Dense(3, activation='softmax')
    ])
    model.compile(
    optimizer = 'adam',
    loss = 'categorical_crossentropy',
    metrics = ['accuracy'])
    model.save("model.h5")
    return model

#model.summary()





from keras.callbacks import EarlyStopping, ReduceLROnPlateau
 
 
#class myCallback(tf.keras.callbacks.Callback):
   # def on_epoch_end(self, epoch, logs={}):
       # if logs.get('val_accuracy') > 0.90:
           # print('\n Validation accuracy has reached uptos \
                   #   90% so, stopping further training.')
          #  self.model.stop_training = True
 
 
es = EarlyStopping(patience=3,
                   monitor='val_accuracy',
                   restore_best_weights=True)
 
lr = ReduceLROnPlateau(monitor='val_loss',
                       patience=2,
                       factor=0.5,
                       verbose=1)

print(X_train.shape)
print(Y_train.shape)
def  fit(model):
    history = model.fit(X_train, 
                        Y_train,
                        validation_data = (X_val, Y_val),
                        batch_size = BATCH_SIZE,
                        epochs = EPOCHS,
                        verbose = 1,
                        )
                        #callbacks = [es, lr, myCallback()])
    return history

#history_df = pd.DataFrame(history.history)
#history_df.loc[:,['loss','val_loss']].plot()
#history_df.loc[:,['accuracy','val_accuracy']].plot()
#plt.show()

#Y_pred = model.predict(X_val)
#Y_val = np.argmax(Y_val, axis=1)
#Y_pred = np.argmax(Y_pred, axis=1)

#metrics.confusion_matrix(Y_val, Y_pred)

#print(metrics.classification_report(Y_val, Y_pred, target_names=classes))
def predictt(a,data_dir):                              
    img = tf.keras.utils.load_img(
        data_dir, target_size=(IMG_SIZE, IMG_SIZE)
    )
    img_array = tf.keras.utils.img_to_array(img)
    img_array = tf.expand_dims(img_array, 0) # Create a batch

    predictions = a.predict(img_array)
    #print(predictions)
    #Y_val = np.argmax(Y_val, axis=1)
    predictions = np.argmax(predictions, axis=0)
    #score=tf.nn.softmax(predictions[0])
    #metrics.confusion_matrix(img_array, predictions)
    return "This image most likely belongs to {} ".format(classes[np.argmax(predictions)])
    